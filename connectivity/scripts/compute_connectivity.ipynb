{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal, stats\n",
    "import re\n",
    "import os\n",
    "import mne\n",
    "import mne_connectivity\n",
    "import IPython\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import joblib\n",
    "import h5io\n",
    "import dask.array as da \n",
    "import itertools\n",
    "\n",
    "import statsmodels\n",
    "from statsmodels import stats\n",
    "from statsmodels.stats import multitest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep paths ##\n",
    "\n",
    "subject = 'BJH027'\n",
    "raw_data_dir = f\"/home/brooke/pacman/raw_data/{subject}\"\n",
    "preproc_data_dir = f\"/home/brooke/pacman/preprocessing/{subject}/ieeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/brooke/pacman/preprocessing/BJH027/ieeg/BJH027_bp_filtered_clean_last_away_events.fif ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79567/1489456339.py:4: RuntimeWarning: This filename (/home/brooke/pacman/preprocessing/BJH027/ieeg/BJH027_bp_filtered_clean_last_away_events.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  last_away_epochs = mne.read_epochs(f\"{preproc_data_dir}/{subject}_bp_filtered_clean_last_away_events.fif\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found the data of interest:\n",
      "        t =   -5000.00 ...    5000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Reading /home/brooke/pacman/preprocessing/BJH027/ieeg/BJH027_bp_filtered_clean_last_away_events-1.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -5000.00 ...    5000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "207 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 5 columns\n"
     ]
    }
   ],
   "source": [
    "## Load Neural Data\n",
    "\n",
    "# load\n",
    "last_away_epochs = mne.read_epochs(f\"{preproc_data_dir}/{subject}_bp_filtered_clean_last_away_events.fif\")\n",
    "\n",
    "# get good epochs (for behavioral data only)\n",
    "good_epochs = [i for i,x in enumerate(last_away_epochs.get_annotations_per_epoch()) if not x]\n",
    "bad_epochs = [i for i,x in enumerate(last_away_epochs.get_annotations_per_epoch()) if  x]\n",
    "\n",
    "# load behavioral data\n",
    "last_away_data = pd.read_csv(f\"{raw_data_dir}/behave/{subject}_last_away_events.csv\")\n",
    "\n",
    "# set info as metadata\n",
    "last_away_epochs.metadata = last_away_data\n",
    "\n",
    "# onlt good epochs\n",
    "last_away_epochs = last_away_epochs[good_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary of electrode locations ##\n",
    "\n",
    "# Dictionary mapping ROI to elecs\n",
    "# Pull mapping ROI to elecs\n",
    "%run /home/brooke/pacman/preprocessing/scripts/roi.py\n",
    "ROIs = ROIs[subject]\n",
    "\n",
    "## prep lists\n",
    "\n",
    "# primary ROI\n",
    "hc_list = []\n",
    "hc_indices = []\n",
    "hc_names = []\n",
    "ofc_list = []\n",
    "ofc_indices = []\n",
    "ofc_names = []\n",
    "amyg_list = []\n",
    "amyg_names = [] \n",
    "amyg_indices = []\n",
    "cing_list = []\n",
    "cing_names = [] \n",
    "cing_indices = []\n",
    "\n",
    "# control ROI\n",
    "insula_list = []\n",
    "insula_names = []  \n",
    "insula_indices = []\n",
    "dlpfc_list = []\n",
    "dlpfc_names = []  \n",
    "dlpfc_indices = []\n",
    "ec_list = []\n",
    "ec_names = []  \n",
    "ec_indices = []\n",
    "\n",
    "# exclude bad ROI from list\n",
    "pairs_long_name = [ch.split('-') for ch in last_away_epochs.info['ch_names']]\n",
    "bidx = len(last_away_epochs.info['bads']) +1\n",
    "pairs_name = pairs_long_name[bidx:len(pairs_long_name)]\n",
    "\n",
    "# sort ROI into lists\n",
    "for ix in range(0, len(pairs_name)):\n",
    "    if pairs_name[ix][0] in ROIs['hc'] or pairs_name[ix][1] in ROIs['hc']:\n",
    "        hc_list.append(last_away_epochs.info['ch_names'][ix + bidx])\n",
    "        hc_names.append(pairs_name[ix])\n",
    "        hc_indices.append(ix)\n",
    "    if pairs_name[ix][0] in ROIs['ofc'] or pairs_name[ix][1] in ROIs['ofc']:\n",
    "        ofc_list.append(last_away_epochs.info['ch_names'][ix + bidx])\n",
    "        ofc_names.append(pairs_name[ix])\n",
    "        ofc_indices.append(ix)\n",
    "    if pairs_name[ix][0] in ROIs['amyg'] or pairs_name[ix][1] in ROIs['amyg']:\n",
    "        amyg_list.append(last_away_epochs.info['ch_names'][ix + bidx])       \n",
    "        amyg_names.append(pairs_name[ix])\n",
    "        amyg_indices.append(ix)\n",
    "    if pairs_name[ix][0] in ROIs['cing'] or pairs_name[ix][1] in ROIs['cing']:\n",
    "        cing_list.append(last_away_epochs.info['ch_names'][ix + bidx])       \n",
    "        cing_names.append(pairs_name[ix])\n",
    "        cing_indices.append(ix)\n",
    "        \n",
    "    # control roi\n",
    "    if pairs_name[ix][0] in ROIs['insula'] or pairs_name[ix][1] in ROIs['insula']:\n",
    "        insula_list.append(last_away_epochs.info['ch_names'][ix + bidx])       \n",
    "        insula_names.append(pairs_name[ix])\n",
    "        insula_indices.append(ix)\n",
    "    if pairs_name[ix][0] in ROIs['dlpfc'] or pairs_name[ix][1] in ROIs['dlpfc']:\n",
    "        dlpfc_list.append(last_away_epochs.info['ch_names'][ix + bidx])       \n",
    "        dlpfc_names.append(pairs_name[ix])\n",
    "        dlpfc_indices.append(ix)\n",
    "    if pairs_name[ix][0] in ROIs['ec'] or pairs_name[ix][1] in ROIs['ec']:\n",
    "        ec_list.append(last_away_epochs.info['ch_names'][ix + bidx])       \n",
    "        ec_names.append(pairs_name[ix])\n",
    "        ec_indices.append(ix)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## funcions\n",
    "\n",
    "def compute_coherence(epochs, ch_names, roi_indices, freqs, n_cycles,  workers = 8):\n",
    "    \"\"\" function to compute TFR via Morlet wavelets\n",
    "    \n",
    "    epochs:                     MNE epoch object with channels of interest\n",
    "    freqs:                      list of frequencies, should be log spaced\n",
    "    n_cycles:                   number of cycles, adjust with freqs to balance temporal and frequency resolution\n",
    "    workers:                    number of threads to use while calculating TFR\n",
    "    \"\"\"\n",
    "    print('computing TFR')\n",
    "    connect = mne_connectivity.spectral_connectivity_epochs(data = epochs,\n",
    "                                                            names = ch_names,\n",
    "                                                            method = ['imcoh', 'ppc', 'wpli2_debiased'],\n",
    "                                                            indices = roi_indices,\n",
    "                                                            mode = 'cwt_morlet',\n",
    "                                                            cwt_freqs = freqs,\n",
    "                                                            cwt_n_cycles = n_cycles,\n",
    "                                                            n_jobs = workers)\n",
    "\n",
    "    return connect\n",
    "\n",
    "\n",
    "\n",
    "def shuffle_epochs(epoch1):\n",
    "    \"\"\"\n",
    "    Shuffles trials in the first epoch object and then combines it with the second epoch object.\n",
    "\n",
    "    Parameters:\n",
    "    epoch1 (mne.Epochs): The first epoch object to be shuffled.\n",
    "\n",
    "    Returns:\n",
    "    mne.Epochs: The shuffled epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Shuffle the first epoch\n",
    "    indices = np.arange(len(epoch1))\n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_epoch1 = epoch1[indices]\n",
    "\n",
    "\n",
    "    return shuffled_epoch1\n",
    "\n",
    "\n",
    "def get_indices_of_connectivity_pairs(roi_lists, ch_names):\n",
    "    \"\"\"\n",
    "    Generates indices of channel names corresponding to unique, non-symmetric pairs \n",
    "    formed from a list of Regions of Interest (ROIs).\n",
    "\n",
    "    Parameters:\n",
    "    roi_lists (list of lists): A list where each element is a list of ROIs (Region of Interest).\n",
    "                               Each sublist represents a different ROI category.\n",
    "    ch_names (list): A list of channel names.\n",
    "\n",
    "    Returns:\n",
    "    tuple of lists: Two lists containing the indices. The first list contains indices from ch_names \n",
    "                    that match the first element of each pair. The second list contains indices from \n",
    "                    ch_names that match the second element of each pair.\n",
    "\n",
    "    Example:\n",
    "    roi_lists = [roi_list1, roi_list2, ...]\n",
    "    ch_names = [\"ch1\", \"ch2\", \"ch3\", ...]\n",
    "    first_pair_indices, second_pair_indices = get_indices_of_connectivity_pairs(roi_lists, ch_names)\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate all unique, non-symmetric pairs from the ROI lists\n",
    "    pairs = [(item1, item2) for i, list1 in enumerate(roi_lists) \n",
    "                            for j, list2 in enumerate(roi_lists) \n",
    "                            if i < j \n",
    "                            for item1, item2 in itertools.product(list1, list2)]\n",
    "\n",
    "    # Find indices in ch_names matching the first element of each pair\n",
    "    first_pair_indices = [idx for pair in pairs \n",
    "                                    for idx, roi in enumerate(ch_names) \n",
    "                                    if roi == pair[0]]\n",
    "\n",
    "    # Find indices in ch_names matching the second element of each pair\n",
    "    second_pair_indices = [idx for pair in pairs \n",
    "                                    for idx, roi in enumerate(ch_names) \n",
    "                                    if roi == pair[1]]\n",
    "    \n",
    "    return first_pair_indices, second_pair_indices\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set frequencies ##\n",
    "\n",
    "freqs = np.logspace(start = np.log10(1), stop = np.log10(150), num = 80, base = 10, endpoint = True)\n",
    "n_cycles = np.logspace(np.log10(2), np.log10(30), base = 10, num = 80)\n",
    "\n",
    "# delta, theta, hfa\n",
    "delta_freqs = freqs[np.where((freqs <= 3))]\n",
    "delta_cycles = n_cycles[np.where((freqs <= 3))]\n",
    "\n",
    "theta_freqs = freqs[np.where((freqs > 3) & (freqs < 8))]\n",
    "theta_cycles = n_cycles[np.where((freqs > 3) & (freqs < 8))]\n",
    "\n",
    "hfa_freqs = freqs[np.where((freqs > 70))]\n",
    "hfa_cycles = n_cycles[np.where((freqs > 70))]\n",
    "\n",
    "# permutations #\n",
    "permutations = 2\n",
    "\n",
    "# resample \n",
    "if last_away_epochs.info['sfreq'] > 1000:\n",
    "    last_away_epochs= last_away_epochs.resample(100)\n",
    "\n",
    "# Crop #\n",
    "last_away_epochs.crop(tmin = -2.5, tmax = 2.5) \n",
    "\n",
    "## remove any electrodes that are duplicated across regions... ugh so much code for such a simple thing ##\n",
    "# Combine all ROI lists into a single Series\n",
    "elec_list = pd.Series(hc_list + amyg_list + ofc_list + cing_list + dlpfc_list + insula_list)\n",
    "\n",
    "# Identify duplicated elements\n",
    "duplicated_list = elec_list[elec_list.duplicated()].tolist()\n",
    "\n",
    "# ROI lists\n",
    "roi_lists = [hc_list, amyg_list, ofc_list, cing_list, dlpfc_list, insula_list]\n",
    "\n",
    "# Find indices of last occurrences of duplicated elements\n",
    "items_to_remove = [(sub_roi, idx_list) for idx_list, sub_list in enumerate(roi_lists) \n",
    "                   for idx, sub_roi in enumerate(sub_list) \n",
    "                   if sub_roi in duplicated_list and \n",
    "                      idx == len(sub_list) - sub_list[::-1].index(sub_roi) - 1]\n",
    "\n",
    "# Convert to list of tuples with unique first elements\n",
    "items_to_remove = list({t[0]: t for t in items_to_remove}.values())\n",
    "\n",
    "# reset roi lists\n",
    "hc_list = roi_lists[0]\n",
    "amyg_list = roi_lists[1]\n",
    "ofc_list = roi_lists[2]\n",
    "cing_list = roi_lists[3]\n",
    "dlpfc_list = roi_lists[4]\n",
    "insula_list = roi_lists[5]\n",
    "roi_lists = [hc_list, amyg_list, ofc_list, cing_list, dlpfc_list, insula_list]\n",
    "\n",
    "# only ROI of interest\n",
    "last_away_hc = last_away_epochs.copy().pick_channels(hc_list)\n",
    "last_away_amyg = last_away_epochs.copy().pick_channels(amyg_list)\n",
    "last_away_ofc = last_away_epochs.copy().pick_channels(ofc_list)\n",
    "last_away_cing = last_away_epochs.copy().pick_channels(cing_list)\n",
    "last_away_dlpfc = last_away_epochs.copy().pick_channels(dlpfc_list)\n",
    "last_away_insula = last_away_epochs.copy().pick_channels(insula_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    roi_coherence = []\n",
    "    last_away_hc = last_away_epochs.copy().pick_channels(hc_list)\n",
    "    last_away_amyg = last_away_epochs.copy().pick_channels(amyg_list)\n",
    "    last_away_ofc = last_away_epochs.copy().pick_channels(ofc_list)\n",
    "    last_away_cing = last_away_epochs.copy().pick_channels(cing_list)\n",
    "    last_away_dlpfc = last_away_epochs.copy().pick_channels(dlpfc_list)\n",
    "    last_away_insula = last_away_epochs.copy().pick_channels(insula_list)\n",
    "\n",
    "    ## shuffle trials ##\n",
    "    last_away_hc = shuffle_epochs(last_away_hc)\n",
    "    last_away_amyg = shuffle_epochs(last_away_amyg)\n",
    "    last_away_ofc = shuffle_epochs(last_away_ofc)\n",
    "    last_away_cing = shuffle_epochs(last_away_cing)\n",
    "    last_away_dlpfc = shuffle_epochs(last_away_dlpfc)\n",
    "    last_away_insula = shuffle_epochs(last_away_insula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for perm in range(0, permutations):\n",
    "\n",
    "    roi_coherence = []\n",
    "    last_away_hc = last_away_epochs.copy().pick_channels(hc_list)\n",
    "    last_away_amyg = last_away_epochs.copy().pick_channels(amyg_list)\n",
    "    last_away_ofc = last_away_epochs.copy().pick_channels(ofc_list)\n",
    "    last_away_cing = last_away_epochs.copy().pick_channels(cing_list)\n",
    "    last_away_dlpfc = last_away_epochs.copy().pick_channels(dlpfc_list)\n",
    "    last_away_insula = last_away_epochs.copy().pick_channels(insula_list)\n",
    "\n",
    "    ## shuffle trials ##\n",
    "    last_away_hc = shuffle_epochs(last_away_hc)\n",
    "    last_away_amyg = shuffle_epochs(last_away_amyg)\n",
    "    last_away_ofc = shuffle_epochs(last_away_ofc)\n",
    "    last_away_cing = shuffle_epochs(last_away_cing)\n",
    "    last_away_dlpfc = shuffle_epochs(last_away_dlpfc)\n",
    "    last_away_insula = shuffle_epochs(last_away_insula)\n",
    "\n",
    "    ## combine ##\n",
    "    last_away_roi = last_away_hc.add_channels([last_away_amyg, last_away_ofc, last_away_cing, last_away_dlpfc, last_away_insula])\n",
    "\n",
    "    ## get indicies for all the noon-symmetric pairs ##\n",
    "    first_pair_indices, second_pair_indices = get_indices_of_connectivity_pairs(roi_lists, last_away_roi.info['ch_names'])\n",
    "\n",
    "    ## compute connectivity ##\n",
    "    roi_coherence = []\n",
    "    roi_coherence = compute_coherence(last_away_roi, last_away_roi.info.ch_names, (hc_index_list, ofc_index_list), theta_freqs, theta_cycles, workers = 8)\n",
    "\n",
    "    # pull out different measures #\n",
    "    imcoh = roi_coherence[0].get_data().mean(axis = 1)\n",
    "    ppc = roi_coherence[1].get_data().mean(axis = 1)\n",
    "    pli = roi_coherence[2].get_data().mean(axis = 1)\n",
    "\n",
    "    if perm == 0:\n",
    "        imcoh_permutations = imcoh.copy()\n",
    "        ppc_permutations = ppc.copy()\n",
    "        pli_permutations = pli.copy()\n",
    "    else:\n",
    "        imcoh_permutations = np.vstack([imcoh_permutations, imcoh])\n",
    "        ppc_permutations = np.vstack([ppc_permutations, ppc])\n",
    "        pli_permutations = np.vstack([pli_permutations, pli])\n",
    "        \n",
    "\n",
    "    if perm % 10 == 0:\n",
    "        np.save('/home/brooke/pacman/across_subject_analyses/ieeg/connectivity/perms/imcoh_perm.npy', imcoh_permutations)\n",
    "        np.save('/home/brooke/pacman/across_subject_analyses/ieeg/connectivity/perms/ppc_perm.npy', ppc_permutations)\n",
    "        np.save('/home/brooke/pacman/across_subject_analyses/ieeg/connectivity/perms/pli_perm.npy', pli_permutations)\n",
    "\n",
    "# final save\n",
    "np.save(f'/home/brooke/pacman/across_subject_analyses/ieeg/connectivity/perms/{subject}_imcoh_perm.npy', imcoh_permutations)\n",
    "np.save(f'/home/brooke/pacman/across_subject_analyses/ieeg/connectivity/perms/{subject}_ppc_perm.npy', ppc_permutations)\n",
    "np.save(f'/home/brooke/pacman/across_subject_analyses/ieeg/connectivity/perms/{subject}_pli_perm.npy', pli_permutations)\n",
    "np.save(f'/home/brooke/pacman/across_subject_analyses/ieeg/connectivity/perms/{subject}_ch_names.npy', last_away_roi.info['ch_names'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieeg_analysis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
