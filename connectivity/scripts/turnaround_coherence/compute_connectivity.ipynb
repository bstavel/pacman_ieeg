{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal, stats\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import mne\n",
    "import mne_connectivity\n",
    "import IPython\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import joblib\n",
    "import h5io\n",
    "import dask.array as da \n",
    "import itertools\n",
    "\n",
    "import statsmodels\n",
    "from statsmodels import stats\n",
    "from statsmodels.stats import multitest\n",
    "\n",
    "# custom functions\n",
    "sys.path.append('/home/brooke/pacman/preprocessing/scripts')\n",
    "from connectivity_functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep paths ##\n",
    "\n",
    "subject = 'BJH039'\n",
    "raw_data_dir = f\"/home/brooke/pacman/raw_data/{subject}\"\n",
    "preproc_data_dir = f\"/home/brooke/pacman/preprocessing/{subject}/ieeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/brooke/pacman/preprocessing/BJH039/ieeg/BJH039_bp_filtered_clean_last_away_events.fif ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_789678/1489456339.py:4: RuntimeWarning: This filename (/home/brooke/pacman/preprocessing/BJH039/ieeg/BJH039_bp_filtered_clean_last_away_events.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  last_away_epochs = mne.read_epochs(f\"{preproc_data_dir}/{subject}_bp_filtered_clean_last_away_events.fif\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found the data of interest:\n",
      "        t =   -5000.00 ...    5000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "178 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 5 columns\n"
     ]
    }
   ],
   "source": [
    "## Load Neural Data\n",
    "\n",
    "# load\n",
    "last_away_epochs = mne.read_epochs(f\"{preproc_data_dir}/{subject}_bp_filtered_clean_last_away_events.fif\")\n",
    "\n",
    "# get good epochs (for behavioral data only)\n",
    "good_epochs = [i for i,x in enumerate(last_away_epochs.get_annotations_per_epoch()) if not x]\n",
    "bad_epochs = [i for i,x in enumerate(last_away_epochs.get_annotations_per_epoch()) if  x]\n",
    "\n",
    "# load behavioral data\n",
    "last_away_data = pd.read_csv(f\"{raw_data_dir}/behave/{subject}_last_away_events.csv\")\n",
    "\n",
    "# set info as metadata\n",
    "last_away_epochs.metadata = last_away_data\n",
    "\n",
    "# onlt good epochs\n",
    "last_away_epochs = last_away_epochs[good_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STI',\n",
       " 'AR1-AR2',\n",
       " 'AR2-AR3',\n",
       " 'AR3-AR4',\n",
       " 'AR4-AR5',\n",
       " 'AR5-AR6',\n",
       " 'AR6-AR7',\n",
       " 'AR7-AR8',\n",
       " 'AR8-AR9',\n",
       " 'AR9-AR10',\n",
       " 'AR10-AR11',\n",
       " 'AR11-AR12',\n",
       " 'BR1-BR2',\n",
       " 'BR2-BR3',\n",
       " 'BR3-BR4',\n",
       " 'BR4-BR5',\n",
       " 'BR5-BR6',\n",
       " 'BR6-BR7',\n",
       " 'BR7-BR8',\n",
       " 'BR8-BR9',\n",
       " 'BR9-BR10',\n",
       " 'BR10-BR11',\n",
       " 'BR11-BR12',\n",
       " 'BR12-BR13',\n",
       " 'BR13-BR14',\n",
       " 'CR1-CR2',\n",
       " 'CR2-CR3',\n",
       " 'CR3-CR4',\n",
       " 'CR4-CR5',\n",
       " 'CR5-CR6',\n",
       " 'CR6-CR7',\n",
       " 'CR7-CR8',\n",
       " 'CR8-CR9',\n",
       " 'CR9-CR10',\n",
       " 'DR1-DR2',\n",
       " 'DR2-DR3',\n",
       " 'DR3-DR4',\n",
       " 'DR4-DR5',\n",
       " 'DR5-DR6',\n",
       " 'DR6-DR7',\n",
       " 'DR7-DR8',\n",
       " 'DR8-DR9',\n",
       " 'DR9-DR10',\n",
       " 'DR10-DR11',\n",
       " 'DR11-DR12',\n",
       " 'ER1-ER2',\n",
       " 'ER2-ER3',\n",
       " 'ER3-ER4',\n",
       " 'ER4-ER5',\n",
       " 'ER5-ER6',\n",
       " 'ER6-ER7',\n",
       " 'ER7-ER8',\n",
       " 'ER8-ER9',\n",
       " 'ER9-ER10',\n",
       " 'ER10-ER11',\n",
       " 'ER11-ER12',\n",
       " 'ER12-ER13',\n",
       " 'ER13-ER14',\n",
       " 'FR1-FR2',\n",
       " 'FR2-FR3',\n",
       " 'FR3-FR4',\n",
       " 'FR4-FR5',\n",
       " 'FR5-FR6',\n",
       " 'FR6-FR7',\n",
       " 'FR7-FR8',\n",
       " 'FR8-FR9',\n",
       " 'FR9-FR10',\n",
       " 'GR2-GR3',\n",
       " 'GR3-GR4',\n",
       " 'GR4-GR5',\n",
       " 'GR5-GR6',\n",
       " 'GR6-GR7',\n",
       " 'GR7-GR8',\n",
       " 'GR8-GR9',\n",
       " 'GR9-GR10',\n",
       " 'GR10-GR11',\n",
       " 'GR11-GR12',\n",
       " 'GR12-GR13',\n",
       " 'GR13-GR14',\n",
       " 'HR1-HR2',\n",
       " 'HR2-HR3',\n",
       " 'HR3-HR4',\n",
       " 'HR4-HR5',\n",
       " 'HR5-HR6',\n",
       " 'HR6-HR7',\n",
       " 'HR7-HR8',\n",
       " 'HR8-HR9',\n",
       " 'HR9-HR10',\n",
       " 'HR10-HR11',\n",
       " 'HR11-HR12',\n",
       " 'IR1-IR2',\n",
       " 'IR2-IR3',\n",
       " 'IR3-IR5',\n",
       " 'IR5-IR6',\n",
       " 'IR6-IR7',\n",
       " 'IR7-IR8',\n",
       " 'IR8-IR9',\n",
       " 'IR9-IR10',\n",
       " 'IR10-IR11',\n",
       " 'IR11-IR12',\n",
       " 'IR12-IR13',\n",
       " 'IR13-IR14',\n",
       " 'JR1-JR2',\n",
       " 'JR2-JR3',\n",
       " 'JR3-JR4',\n",
       " 'JR4-JR5',\n",
       " 'JR5-JR6',\n",
       " 'JR6-JR7',\n",
       " 'JR7-JR8',\n",
       " 'KL6-KL7',\n",
       " 'KL7-KL8',\n",
       " 'KL8-KL9',\n",
       " 'KL9-KL10',\n",
       " 'KL10-KL11',\n",
       " 'KL11-KL12',\n",
       " 'LL6-LL7',\n",
       " 'LL7-LL8',\n",
       " 'LL8-LL9',\n",
       " 'LL9-LL11',\n",
       " 'ML5-ML6',\n",
       " 'ML6-ML7',\n",
       " 'ML7-ML8',\n",
       " 'ML8-ML9',\n",
       " 'ML9-ML10',\n",
       " 'ML10-ML11',\n",
       " 'ML11-ML12']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_away_epochs.info['ch_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary of electrode locations ##\n",
    "\n",
    "# Dictionary mapping ROI to elecs\n",
    "# Pull mapping ROI to elecs\n",
    "%run /home/brooke/pacman/preprocessing/scripts/roi.py\n",
    "ROIs = ROIs[subject]\n",
    "\n",
    "## prep lists\n",
    "\n",
    "# primary ROI\n",
    "hc_list = []\n",
    "hc_indices = []\n",
    "hc_names = []\n",
    "ofc_list = []\n",
    "ofc_indices = []\n",
    "ofc_names = []\n",
    "amyg_list = []\n",
    "amyg_names = [] \n",
    "amyg_indices = []\n",
    "cing_list = []\n",
    "cing_names = [] \n",
    "cing_indices = []\n",
    "\n",
    "# control ROI\n",
    "insula_list = []\n",
    "insula_names = []  \n",
    "insula_indices = []\n",
    "dlpfc_list = []\n",
    "dlpfc_names = []  \n",
    "dlpfc_indices = []\n",
    "ec_list = []\n",
    "ec_names = []  \n",
    "ec_indices = []\n",
    "\n",
    "# exclude bad ROI from list\n",
    "pairs_long_name = [ch.split('-') for ch in last_away_epochs.info['ch_names']]\n",
    "bidx = len(last_away_epochs.info['bads']) +1\n",
    "pairs_name = pairs_long_name[bidx:len(pairs_long_name)]\n",
    "\n",
    "# sort ROI into lists\n",
    "for ix in range(0, len(pairs_name)):\n",
    "    if pairs_name[ix][0] in ROIs['hc'] or pairs_name[ix][1] in ROIs['hc']:\n",
    "        hc_list.append(last_away_epochs.info['ch_names'][ix + bidx])\n",
    "        hc_names.append(pairs_name[ix])\n",
    "        hc_indices.append(ix)\n",
    "    if pairs_name[ix][0] in ROIs['ofc'] or pairs_name[ix][1] in ROIs['ofc']:\n",
    "        ofc_list.append(last_away_epochs.info['ch_names'][ix + bidx])\n",
    "        ofc_names.append(pairs_name[ix])\n",
    "        ofc_indices.append(ix)\n",
    "    if pairs_name[ix][0] in ROIs['amyg'] or pairs_name[ix][1] in ROIs['amyg']:\n",
    "        amyg_list.append(last_away_epochs.info['ch_names'][ix + bidx])       \n",
    "        amyg_names.append(pairs_name[ix])\n",
    "        amyg_indices.append(ix)\n",
    "    if pairs_name[ix][0] in ROIs['cing'] or pairs_name[ix][1] in ROIs['cing']:\n",
    "        cing_list.append(last_away_epochs.info['ch_names'][ix + bidx])       \n",
    "        cing_names.append(pairs_name[ix])\n",
    "        cing_indices.append(ix)\n",
    "        \n",
    "    # control roi\n",
    "    if pairs_name[ix][0] in ROIs['insula'] or pairs_name[ix][1] in ROIs['insula']:\n",
    "        insula_list.append(last_away_epochs.info['ch_names'][ix + bidx])       \n",
    "        insula_names.append(pairs_name[ix])\n",
    "        insula_indices.append(ix)\n",
    "    if pairs_name[ix][0] in ROIs['dlpfc'] or pairs_name[ix][1] in ROIs['dlpfc']:\n",
    "        dlpfc_list.append(last_away_epochs.info['ch_names'][ix + bidx])       \n",
    "        dlpfc_names.append(pairs_name[ix])\n",
    "        dlpfc_indices.append(ix)\n",
    "    if pairs_name[ix][0] in ROIs['ec'] or pairs_name[ix][1] in ROIs['ec']:\n",
    "        ec_list.append(last_away_epochs.info['ch_names'][ix + bidx])       \n",
    "        ec_names.append(pairs_name[ix])\n",
    "        ec_indices.append(ix)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GR2-GR3']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amyg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## funcions\n",
    "\n",
    "def compute_coherence(epochs, ch_names, roi_indices, freqs, n_cycles,  workers = 8):\n",
    "    \"\"\" function to compute TFR via Morlet wavelets\n",
    "    \n",
    "    epochs:                     MNE epoch object with channels of interest\n",
    "    freqs:                      list of frequencies, should be log spaced\n",
    "    n_cycles:                   number of cycles, adjust with freqs to balance temporal and frequency resolution\n",
    "    workers:                    number of threads to use while calculating TFR\n",
    "    \"\"\"\n",
    "    print('computing TFR')\n",
    "    connect = mne_connectivity.spectral_connectivity_epochs(data = epochs,\n",
    "                                                            names = ch_names,\n",
    "                                                            method = ['imcoh', 'ppc', 'wpli2_debiased'],\n",
    "                                                            indices = roi_indices,\n",
    "                                                            mode = 'cwt_morlet',\n",
    "                                                            cwt_freqs = freqs,\n",
    "                                                            cwt_n_cycles = n_cycles,\n",
    "                                                            n_jobs = workers)\n",
    "\n",
    "    return connect\n",
    "\n",
    "\n",
    "\n",
    "def shuffle_epochs(epoch1):\n",
    "    \"\"\"\n",
    "    Shuffles trials in the first epoch object and then combines it with the second epoch object.\n",
    "\n",
    "    Parameters:\n",
    "    epoch1 (mne.Epochs): The first epoch object to be shuffled.\n",
    "\n",
    "    Returns:\n",
    "    mne.Epochs: The shuffled epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Shuffle the first epoch\n",
    "    indices = np.arange(len(epoch1))\n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_epoch1 = epoch1[indices]\n",
    "\n",
    "\n",
    "    return shuffled_epoch1\n",
    "\n",
    "\n",
    "def get_indices_of_connectivity_pairs(roi_lists, ch_names):\n",
    "    \"\"\"\n",
    "    Generates indices of channel names corresponding to unique, non-symmetric pairs \n",
    "    formed from a list of Regions of Interest (ROIs).\n",
    "\n",
    "    Parameters:\n",
    "    roi_lists (list of lists): A list where each element is a list of ROIs (Region of Interest).\n",
    "                               Each sublist represents a different ROI category.\n",
    "    ch_names (list): A list of channel names.\n",
    "\n",
    "    Returns:\n",
    "    tuple of lists: Two lists containing the indices. The first list contains indices from ch_names \n",
    "                    that match the first element of each pair. The second list contains indices from \n",
    "                    ch_names that match the second element of each pair.\n",
    "\n",
    "    Example:\n",
    "    roi_lists = [roi_list1, roi_list2, ...]\n",
    "    ch_names = [\"ch1\", \"ch2\", \"ch3\", ...]\n",
    "    first_pair_indices, second_pair_indices = get_indices_of_connectivity_pairs(roi_lists, ch_names)\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate all unique, non-symmetric pairs from the ROI lists\n",
    "    pairs = [(item1, item2) for i, list1 in enumerate(roi_lists) \n",
    "                            for j, list2 in enumerate(roi_lists) \n",
    "                            if i < j \n",
    "                            for item1, item2 in itertools.product(list1, list2)]\n",
    "\n",
    "    # Find indices in ch_names matching the first element of each pair\n",
    "    first_pair_indices = [idx for pair in pairs \n",
    "                                    for idx, roi in enumerate(ch_names) \n",
    "                                    if roi == pair[0]]\n",
    "\n",
    "    # Find indices in ch_names matching the second element of each pair\n",
    "    second_pair_indices = [idx for pair in pairs \n",
    "                                    for idx, roi in enumerate(ch_names) \n",
    "                                    if roi == pair[1]]\n",
    "    \n",
    "    return first_pair_indices, second_pair_indices\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set frequencies ##\n",
    "\n",
    "freqs = np.logspace(start = np.log10(1), stop = np.log10(150), num = 80, base = 10, endpoint = True)\n",
    "n_cycles = np.logspace(np.log10(2), np.log10(30), base = 10, num = 80)\n",
    "\n",
    "# delta, theta, hfa\n",
    "delta_freqs = freqs[np.where((freqs <= 3))]\n",
    "delta_cycles = n_cycles[np.where((freqs <= 3))]\n",
    "\n",
    "theta_freqs = freqs[np.where((freqs > 3) & (freqs < 8))]\n",
    "theta_cycles = n_cycles[np.where((freqs > 3) & (freqs < 8))]\n",
    "\n",
    "hfa_freqs = freqs[np.where((freqs > 70))]\n",
    "hfa_cycles = n_cycles[np.where((freqs > 70))]\n",
    "\n",
    "# permutations #\n",
    "permutations = 1000\n",
    "\n",
    "# resample \n",
    "if last_away_epochs.info['sfreq'] > 1000:\n",
    "    last_away_epochs= last_away_epochs.resample(100)\n",
    "\n",
    "# Crop #\n",
    "last_away_epochs.crop(tmin = -2.5, tmax = 2.5) \n",
    "\n",
    "## remove any electrodes that are duplicated across regions... ugh so much code for such a simple thing ##\n",
    "# Combine all ROI lists into a single Series\n",
    "elec_list = pd.Series(hc_list + amyg_list + ofc_list + cing_list + dlpfc_list)\n",
    "\n",
    "# Identify duplicated elements\n",
    "duplicated_list = elec_list[elec_list.duplicated()].tolist()\n",
    "\n",
    "# ROI lists\n",
    "roi_lists = [hc_list, amyg_list, ofc_list, cing_list, dlpfc_list]\n",
    "\n",
    "# Find indices of last occurrences of duplicated elements\n",
    "items_to_remove = [(sub_roi, idx_list) for idx_list, sub_list in enumerate(roi_lists) \n",
    "                for idx, sub_roi in enumerate(sub_list) \n",
    "                if sub_roi in duplicated_list and \n",
    "                    idx == len(sub_list) - sub_list[::-1].index(sub_roi) - 1]\n",
    "\n",
    "# Convert to list of tuples with unique first elements\n",
    "items_to_remove = list({t[0]: t for t in items_to_remove}.values())\n",
    "\n",
    "# remove elements\n",
    "for item in items_to_remove:\n",
    "    roi_lists[item[1]].remove(item[0])\n",
    "\n",
    "# reset roi lists\n",
    "hc_list = roi_lists[0]\n",
    "amyg_list = roi_lists[1]\n",
    "ofc_list = roi_lists[2]\n",
    "cing_list = roi_lists[3]\n",
    "dlpfc_list = roi_lists[4]\n",
    "roi_lists = [hc_list, amyg_list, ofc_list, cing_list, dlpfc_list]\n",
    "\n",
    "# only ROI of interest\n",
    "last_away_hc = last_away_epochs.copy().pick_channels(hc_list)\n",
    "last_away_amyg = last_away_epochs.copy().pick_channels(amyg_list)\n",
    "last_away_ofc = last_away_epochs.copy().pick_channels(ofc_list)\n",
    "last_away_cing = last_away_epochs.copy().pick_channels(cing_list)\n",
    "last_away_dlpfc = last_away_epochs.copy().pick_channels(dlpfc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amyg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set frequencies ##\n",
    "\n",
    "freqs = np.logspace(start = np.log10(1), stop = np.log10(150), num = 80, base = 10, endpoint = True)\n",
    "n_cycles = np.logspace(np.log10(2), np.log10(30), base = 10, num = 80)\n",
    "\n",
    "# delta, theta, hfa\n",
    "delta_freqs = freqs[np.where((freqs <= 3))]\n",
    "delta_cycles = n_cycles[np.where((freqs <= 3))]\n",
    "\n",
    "theta_freqs = freqs[np.where((freqs > 3) & (freqs < 8))]\n",
    "theta_cycles = n_cycles[np.where((freqs > 3) & (freqs < 8))]\n",
    "\n",
    "hfa_freqs = freqs[np.where((freqs > 70))]\n",
    "hfa_cycles = n_cycles[np.where((freqs > 70))]\n",
    "\n",
    "# permutations #\n",
    "permutations = 2\n",
    "\n",
    "# resample \n",
    "if last_away_epochs.info['sfreq'] > 1000:\n",
    "    last_away_epochs= last_away_epochs.resample(100)\n",
    "\n",
    "# Crop #\n",
    "last_away_epochs.crop(tmin = -2.5, tmax = 2.5) \n",
    "\n",
    "## remove any electrodes that are duplicated across regions... ugh so much code for such a simple thing ##\n",
    "# Combine all ROI lists into a single Series\n",
    "elec_list = pd.Series(hc_list + amyg_list + ofc_list + cing_list + dlpfc_list + insula_list)\n",
    "\n",
    "# Identify duplicated elements\n",
    "duplicated_list = elec_list[elec_list.duplicated()].tolist()\n",
    "\n",
    "# ROI lists\n",
    "roi_lists = [hc_list, amyg_list, ofc_list, cing_list, dlpfc_list, insula_list]\n",
    "\n",
    "# Find indices of last occurrences of duplicated elements\n",
    "items_to_remove = [(sub_roi, idx_list) for idx_list, sub_list in enumerate(roi_lists) \n",
    "                   for idx, sub_roi in enumerate(sub_list) \n",
    "                   if sub_roi in duplicated_list and \n",
    "                      idx == len(sub_list) - sub_list[::-1].index(sub_roi) - 1]\n",
    "\n",
    "# Convert to list of tuples with unique first elements\n",
    "items_to_remove = list({t[0]: t for t in items_to_remove}.values())\n",
    "\n",
    "# reset roi lists\n",
    "hc_list = roi_lists[0]\n",
    "amyg_list = roi_lists[1]\n",
    "ofc_list = roi_lists[2]\n",
    "cing_list = roi_lists[3]\n",
    "dlpfc_list = roi_lists[4]\n",
    "insula_list = roi_lists[5]\n",
    "roi_lists = [hc_list, amyg_list, ofc_list, cing_list, dlpfc_list, insula_list]\n",
    "\n",
    "# only ROI of interest\n",
    "last_away_hc = last_away_epochs.copy().pick_channels(hc_list)\n",
    "last_away_amyg = last_away_epochs.copy().pick_channels(amyg_list)\n",
    "last_away_ofc = last_away_epochs.copy().pick_channels(ofc_list)\n",
    "last_away_cing = last_away_epochs.copy().pick_channels(cing_list)\n",
    "last_away_dlpfc = last_away_epochs.copy().pick_channels(dlpfc_list)\n",
    "last_away_insula = last_away_epochs.copy().pick_channels(insula_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amyg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_away_hc = last_away_epochs.copy().pick_channels(hc_list)\n",
    "        \n",
    "last_away_roi = last_away_hc.add_channels([last_away_ofc, last_away_cing, last_away_dlpfc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    roi_coherence = []\n",
    "    last_away_hc = last_away_epochs.copy().pick_channels(hc_list)\n",
    "    last_away_amyg = last_away_epochs.copy().pick_channels(amyg_list)\n",
    "    last_away_ofc = last_away_epochs.copy().pick_channels(ofc_list)\n",
    "    last_away_cing = last_away_epochs.copy().pick_channels(cing_list)\n",
    "    last_away_dlpfc = last_away_epochs.copy().pick_channels(dlpfc_list)\n",
    "    last_away_insula = last_away_epochs.copy().pick_channels(insula_list)\n",
    "\n",
    "    ## shuffle trials ##\n",
    "    last_away_hc = shuffle_epochs(last_away_hc)\n",
    "    last_away_amyg = shuffle_epochs(last_away_amyg)\n",
    "    last_away_ofc = shuffle_epochs(last_away_ofc)\n",
    "    last_away_cing = shuffle_epochs(last_away_cing)\n",
    "    last_away_dlpfc = shuffle_epochs(last_away_dlpfc)\n",
    "    last_away_insula = shuffle_epochs(last_away_insula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for perm in range(0, permutations):\n",
    "\n",
    "    roi_coherence = []\n",
    "    last_away_hc = last_away_epochs.copy().pick_channels(hc_list)\n",
    "    last_away_amyg = last_away_epochs.copy().pick_channels(amyg_list)\n",
    "    last_away_ofc = last_away_epochs.copy().pick_channels(ofc_list)\n",
    "    last_away_cing = last_away_epochs.copy().pick_channels(cing_list)\n",
    "    last_away_dlpfc = last_away_epochs.copy().pick_channels(dlpfc_list)\n",
    "    last_away_insula = last_away_epochs.copy().pick_channels(insula_list)\n",
    "\n",
    "    ## shuffle trials ##\n",
    "    last_away_hc = shuffle_epochs(last_away_hc)\n",
    "    last_away_amyg = shuffle_epochs(last_away_amyg)\n",
    "    last_away_ofc = shuffle_epochs(last_away_ofc)\n",
    "    last_away_cing = shuffle_epochs(last_away_cing)\n",
    "    last_away_dlpfc = shuffle_epochs(last_away_dlpfc)\n",
    "    last_away_insula = shuffle_epochs(last_away_insula)\n",
    "\n",
    "    ## combine ##\n",
    "    last_away_roi = last_away_hc.add_channels([last_away_amyg, last_away_ofc, last_away_cing, last_away_dlpfc, last_away_insula])\n",
    "\n",
    "    ## get indicies for all the noon-symmetric pairs ##\n",
    "    first_pair_indices, second_pair_indices = get_indices_of_connectivity_pairs(roi_lists, last_away_roi.info['ch_names'])\n",
    "\n",
    "    ## compute connectivity ##\n",
    "    roi_coherence = []\n",
    "    roi_coherence = compute_coherence(last_away_roi, last_away_roi.info.ch_names, (hc_index_list, ofc_index_list), theta_freqs, theta_cycles, workers = 8)\n",
    "\n",
    "    # pull out different measures #\n",
    "    imcoh = roi_coherence[0].get_data().mean(axis = 1)\n",
    "    ppc = roi_coherence[1].get_data().mean(axis = 1)\n",
    "    pli = roi_coherence[2].get_data().mean(axis = 1)\n",
    "\n",
    "    if perm == 0:\n",
    "        imcoh_permutations = imcoh.copy()\n",
    "        ppc_permutations = ppc.copy()\n",
    "        pli_permutations = pli.copy()\n",
    "    else:\n",
    "        imcoh_permutations = np.vstack([imcoh_permutations, imcoh])\n",
    "        ppc_permutations = np.vstack([ppc_permutations, ppc])\n",
    "        pli_permutations = np.vstack([pli_permutations, pli])\n",
    "        \n",
    "\n",
    "    if perm % 10 == 0:\n",
    "        np.save('/home/brooke/pacman/across_subject_analyses/ieeg/connectivity/perms/imcoh_perm.npy', imcoh_permutations)\n",
    "        np.save('/home/brooke/pacman/across_subject_analyses/ieeg/connectivity/perms/ppc_perm.npy', ppc_permutations)\n",
    "        np.save('/home/brooke/pacman/across_subject_analyses/ieeg/connectivity/perms/pli_perm.npy', pli_permutations)\n",
    "\n",
    "# final save\n",
    "np.save(f'/home/brooke/pacman/across_subject_analyses/ieeg/connectivity/perms/{subject}_imcoh_perm.npy', imcoh_permutations)\n",
    "np.save(f'/home/brooke/pacman/across_subject_analyses/ieeg/connectivity/perms/{subject}_ppc_perm.npy', ppc_permutations)\n",
    "np.save(f'/home/brooke/pacman/across_subject_analyses/ieeg/connectivity/perms/{subject}_pli_perm.npy', pli_permutations)\n",
    "np.save(f'/home/brooke/pacman/across_subject_analyses/ieeg/connectivity/perms/{subject}_ch_names.npy', last_away_roi.info['ch_names'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieeg_analysis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
